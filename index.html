<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings">
  <meta property="og:title" content="MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings"/>
  <meta property="og:description" content="Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; limited diversity in training objectives and data; and scalability issues due to reliance on high-quality labeled paired data for contrastive learning. To address these, we propose \ours{}, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional, context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method resolves the stated limitations by introducing bidirectional attention via continual pre-training, employing diverse multimodal data for enhanced representation robustness, and scaling effectively with massive unlabeled datasets through joint reconstruction objectives.
  Experiments demonstrate that \ours{} consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data.
"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="continual pre-training, multimodal, embedding models, contrastive learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MoCa: <u>Mo</u>dality-aware <u>C</u>ontinual Pre-tr<u>a</u>ining Makes Better Bidirectional Multimodal Embeddings</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><sup>1</sup><a href="https://haon-chen.github.io/" target="_blank">Haonan Chen</a><sup>*</sup>,</span>
              <span class="author-block"><sup>2</sup><a href="https://liuhong99.github.io/" target="_blank">Hong Liu</a><sup>*</sup>,</span>
              <span class="author-block"><sup>3</sup><a href="https://yuping.me/" target="_blank">Yuping Luo</a><sup>*</sup>,</span>
              <span class="author-block"><sup>4</sup>Liang Wang,</span>
              <span class="author-block"><sup>4</sup>Nan Yang,</span>
              <span class="author-block"><sup>4</sup><a href="https://thegenerality.com/" target="_blank">Furu Wei</a>,</span>
              <span class="author-block"><sup>1</sup><a href="http://playbigdata.ruc.edu.cn/dou/" target="_blank">Zhicheng Dou</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block"><br>Conferance name and year</span> -->
              <!-- <br> -->
              <span class="author-block"><sup>1</sup> Gaoling School of Artificial Intelligence, Renmin University of China, <code>{hnchen,dou}@ruc.edu.cn</code></span>
              <br>
              <span class="author-block"><sup>2</sup> <code>hliu99@cs.stanford.edu</code></span>
              <br>
              <span class="author-block"><sup>3</sup> <code>yupingl@cs.princeton.edu</code></span>
              <br>
              <span class="author-block"><sup>4</sup> Microsoft Corporation, <code>{wangliang,nanya,fuwei}@microsoft.com</code></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- HF model link -->
                <span class="link-block">
                  <a href="https://huggingface.co/???" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">ðŸ¤—</span>
                      <span>MoCa models</span>
                  </a>
                </span>

                <!-- HF dataset link -->
                <span class="link-block">
                  <a href="https://huggingface.co/???" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">ðŸ¤—</span>
                      <span>MoCa dataset</span>
                  </a>
                </span>

                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/haon-chen/MoCa" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/videos/banner_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; limited diversity in training objectives and data; and scalability issues due to reliance on high-quality labeled paired data for contrastive learning. To address these, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional, context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment.
              Our method resolves the stated limitations by introducing bidirectional attention via continual pre-training, employing diverse multimodal data for enhanced representation robustness, and scaling effectively with massive unlabeled datasets through joint reconstruction objectives.
              Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <img src="static/images/fig1l.png" alt="Previous causal multimodal embeddings" style="height: 300px;"/>
          <img src="static/images/fig1r.png" alt="MoCa" style="height: 300px;"/>
        </div>
        <!-- <h2 class="subtitle has-text-centered">
          Comparison of VLM-based multimodal embedding models.
        </h2> -->
      </div>
    </div>
  </section>

  <!-- MoCa method -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">MoCa: <u>Mo</u>dality-aware <u>C</u>ontinual Pre-tr<u>a</u>ining</h2>
          <div class="content has-text-justified">
            <p>
              Starting from pre-trained VLMs, our approach consists of two stages:
              <ol>
                <li><strong>Modality-aware Continual Pre-training</strong>: We introduce a joint reconstruction objective that requires the model to simultaneously denoise interleaved text and image inputs, which encourages the model to jointly reason across modalities.
                  <ol>
                    <li><strong>For text</strong>: We apply masked language modeling (MLM), where masked tokens are predicted using the full multimodal context.</li>
                    <li><strong>For images</strong>: We adopt a masked autoencoding (MAE) strategy: a subset of image patches is randomly masked and reconstructed by a lightweight decoder conditioned on image and text contexts.</li>
                  </ol>
                </li>
                <li><strong>Heterogeneous Contrastive Fine-tuning</strong>: As opposed to previous works which used mainly image-caption pairs, we add diverse heterogeneous data including:
                  <ol>
                    <li><strong>Long-form query-document pairs</strong>: Supporting document-level understanding and complex reasoning over extended context</li>
                    <li><strong>Curated multimodal pairs</strong>: Offering various visual and textual contexts beyond image captions of specific distributions</li>
                    <li><strong>Real-world text pairs</strong>: Enhancing linguistic representations across diverse domains</li>
                  </ol>
                </li>
              </ol>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <img src="static/images/fig2.png" alt="MoCa"/>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Overall Results -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overall Results</h2>
          <div class="content has-text-justified">
            <p>
              We present the general multimodal embedding performances on MMEB and the long-form document-level retrieval performances.
              MoCa surpasses all strong baselines on both benchmarks, demonstrating the effectiveness of our training pipeline in improving multimodal embedding quality.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              Below are the results on the MMEB benchmark, which includes 36 tasks across four categories: Classification, Visual Question Answering (VQA), Retrieval, and Visual Grounding.
              In addition to existing baselines, we train three variants of Qwen-2.5-VL with different model sizes and attention types.
              We highlight the best scores in <strong>bold</strong> and the second-best scores with an <u>underline</u>.
            </p>
            <img src="static/images/mteb.png" alt="mteb"/>
            <p>
              Below are the results on the ViDoRe-v2 benchmark, which includes 7 tasks.
            "Syn" is short for synthetic, "Mul" is short for multilingual, and "Bio" is short for Biomedical.
            We highlight the best scores in <strong>bold</strong> and the second-best scores with an <u>underline</u>.
            </p>
            <img src="static/images/vidore_v2.png" alt="vidore"/>
            <p>Several key findings emerge from the results:</p>
            <ol>
                <li>The combination of a VLM backbone with bidirectional attention, continual pretraining (CPT), and heterogeneous contrastive learning (CL) consistently outperforms other variants.
                Specifically, we find that <strong>"bidirectional + CPT + CL"</strong> yields better results than <strong>"causal + CL"</strong>, which itself outperforms <strong>"bidirectional + CL"</strong> without CPT.
                This confirms the importance of modality-aware pretraining in unlocking the full potential of bidirectional architectures.</li>
                <li>After applying continual pretraining for 30B tokens, our 3B model with bidirectional adaptation achieves better performance than 7B baselines trained with only contrastive learning.</li>
                <li>Scaling our approach to a 7B model leads to clear gains across all task categories in MMEB, setting new state-of-the-art results.</li>
                <li>On Vidore, our 3B model achieves the best average performance despite using significantly fewer parameters.
                This demonstrates the effectiveness of our training strategy in producing robust embeddings.</li>
            </ol>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Ablation studies -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Ablation Studies</h2>
          <div class="content has-text-justified">
            <p>
              To understand the contribution of each major design choice in our framework, we conduct ablation studies on both the <em>Modality-aware Continual Pre-training</em> and <em>Heterogeneous Contrastive Fine-tuning</em> stages.
              As shown in in the following table, removing any key component leads to a consistent performance drop on both benchmarks (MMEB and Vidore-v2), demonstrating the importance of each part.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/ablation.png" alt="ablation studies" style="height: 250px;"/>
          <div class="content has-text-justified">
            <p><strong>Modality-aware Continual Pre-training.</strong> The ablation of either the <em>Masked Language Modeling</em> (MLM) or the <em>Masked Autoencoding</em> (MAE) objective results in a performance decline, indicating that both text and image reconstruction are essential for learning modality-specific representations.
            Disabling <em>both</em> yields the largest decline, demonstrating the joint effectiveness between MLM and MAE in training a robust bidirectional encoder.</p>

            <p><strong>Heterogeneous Contrastive Fine-tuning.</strong> We then evaluate the effect of different types of training data used during contrastive fine-tuning.
            Removing <em>text-only pairs</em> results in a noticeable drop, showing their importance for maintaining strong language representations.
            Excluding <em>long-form document retrieval pairs</em> (VisRAG and the training set of ColPali) also hurts performance, especially on ViDoRe, demonstrating their value in supporting deeper reasoning over extended contexts.
            Finally, removing the <em>task-aware batching</em> technique, where data from different tasks are jointly trained in each batch, leads to performance degradation.
            This technique helps prevent the model from overfitting to task-specific patterns and encourages it to learn more sample-discriminative representations, which is crucial for generalization across diverse task formats.</p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Scaling law -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Data Scaling Effect for Continual Pre-training</h2>
          <div class="content has-text-justified">
            <p>
              To assess how the data scale of CPT affects the quality of the learned embeddings, we investigate the
              scaling effect of our framework. We conduct contrastive fine-tuning (CL) using checkpoints saved at
              different steps during a single CPT run for both 3B and 7B models.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <img src="static/images/scaling_law.png" alt="scaling law"/>
            <p>
              As shown in the figure, the downstream performance on MMEB steadily improves as the number of
              CPT steps increases. This demonstrates that modality-aware pre-training helps the model develop
              stronger bidirectional representations, which are then more effectively aligned during contrastive
              fine-tuning. The results demonstrate that CPT provides a progressively better initialization for CL,
              leading to more robust and transferable multimodal embeddings.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre>
          <code>
@article{chen2025moca,
  title={MoCa: Multimodal Contrastive Learning for Visual and Language Understanding},
  author={},
  journal={},
  year={2025}
}
          </code>
        </pre>
      </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
